{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Figure thinking 10APR18\"\noutput: html_notebook\n---\n\nThis notebook details some figure design and sketch of analyses for displaying our main results.\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(ggplot2)\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\nlibrary(here)\nlibrary(dplyr)\nlibrary(MASS)\n\n# set ggplot theme globally:\nsource(here(\"plt_themes.r\"))\ntheme_set(theme_pth1())\n```\n\n\n## Part 1: data load and fitness parsing\n\nHere we load up the data from Milo's latest fitness calculation routing. Data were downloaded as:\n\n```\ncd ~/Dropbox/PLT/analysis_PLT/PLT/data/\ngrdrive download 14Ny9xpvCbPPIWbJsHc5f1imLHIpspSiq --recursive\n```\n\nFirst, we load the data for each BFA, using the cutoff A/T run of 5:\n```{r}\ndBFA2 <- read.csv(here(\"data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/dBFA2_s_03_23_18_GC_cutoff_5.csv\"))\nhBFA1 <- read.csv(here(\"data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/hBFA1_s_03_23_18_GC_cutoff_5.csv\"))\nhBFA2 <- read.csv(here(\"data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/hBFA2_s_03_23_18_GC_cutoff_5.csv\"))\n```\n\nFor determining adaptedness: fit $t$ distribution for each environment and empirically calculate the quantile distribution and the correlation matrix among all BCs in the putative adaptive set.\n\nLet's start with dBFA2. Fitness columns contain the string `iva_s`, so pull out indexes for each of these:\n```{r}\n# grab columns corresponding to inverse variance weighted (by rep) average fitness estimates\nd2cols <- grep('iva_s', names(dBFA2), value = T)\n\n# grab cols corresponding to the variance of these estimates\nd2cols_err <- grep('_err', d2cols, value = T)\n\nmeta_cols <- names(dBFA2)[1:5] # define meta-data for BCs to capture\ndBFA2_2     <- dBFA2[,(names(dBFA2) %in% c(meta_cols, d2cols)) & (!names(dBFA2) %in% d2cols_err)]\ndBFA2_2_err <- dBFA2[,names(dBFA2) %in% c(meta_cols, d2cols_err)]\n```\n\nUse this to evaluate the neutrals across the environments. First let's plot them to see what's up. First let's produce long-form data so we can plot this stuff using `ggplot2`\n\n```{r, message=FALSE, warning=FALSE}\n# re-name columns:\nd2cols2 <- gsub('.iva_s','', d2cols)\nd3 <- reshape2::melt(dBFA2_2, value.name = 'iva.s', variable.name = 'bfa.env')\n\n# bring over the error col as well\nd3_err <- reshape2::melt(dBFA2_2_err, value.name = 'iva.s_err', variable.name = 'bfa.env')\n\n# re-name bfa.env without 'iva_s'\nd3$bfa.env <- gsub('\\\\.iva_s', '', d3$bfa.env)\nd3_err$bfa.env <- gsub('\\\\.iva_s_err', '', d3_err$bfa.env)\n\n# merge the two:\nd4 <- merge(d3,d3_err,sort = F)\n\n# change how iva.s and iva.s_err are reported, from per-cycle to per-generation:\nd4$iva.s <- d4$iva.s / 8\nd4$iva.s_err <- d4$iva.s_err / 8\n```\n\nThe neutral set for dBFA2 corresponds to `d3$Subpool.Environment == 'Ancestor_YPD_2N'`. Let's capture these and plot their fitnesses across environments:\n```{r}\nd4n <- d4[d4$Subpool.Environment == 'Ancestor_YPD_2N',]\n\n# calculate mean and sd (face value) of iva.s:\nggplot(d4n, aes(x = iva.s)) + geom_density() + facet_wrap( ~ bfa.env, scales = \"free\") + theme_bw()\n```\n\nCheck if any entries are `NA`:\n```{r}\nsum(is.na(d4n$iva.s))\n```\n\nMost of these probably occur in `CLM`. Let's check:\n```{r}\ntable(d4n[is.na(d4n$iva.s)==TRUE,'bfa.env'])[table(d4n[is.na(d4n$iva.s)==TRUE,'bfa.env'])>0]\n```\n\nYep, OK. Basically, neutral clones go extinct or otherwise have terrible fitness estimates in these drug environments. To deal with this, we'll need to estimate a lower bound on fitness based on the read count data and assign these guys some fitness based on that. For now, however, we'll do something super cludgey, which is to assign these BCs a random fitness within $t$-distribution defined by the neutral set.\n\nSo, finally, let's plot these lineages, having initially removed these `NA` lineages:\n```{r, fig.height=3, fig.width=5, message=FALSE, warning=FALSE}\nd4n2 <- d4n[complete.cases(d4n),]\nggplot(d4n2, aes(x = bfa.env, y = iva.s, group = Full.BC, color = Which.Subpools)) + \n  geom_line() + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n```\n\nWe have a *clear* problem with fitness estimates for these lineages from the drug environment. The `R2` ancestral clones were spiked in a ten-fold lower frequency than the `R1` lineages, I believe, which may explain the larger than average swings in this sub-group.\n\nLet's pretend this problem doesn't exist. We can calculate (by hand) the fitness estimates for the drug environments by calculating the posterior of the logit slopes between $t_8$ and $t_16$ (one transfer) and use the posteriors as a weighting factor to average the observed $s_{neut}^{env}$. We can probably do this rather quickly..\n\nLet's spend a few minutes seeing whether we can do this, so that we have a roughly complete dataset to play with for generating figures. Loading up the BFA count data:\n\nFrom the shell:\n```\ncd ~/Dropbox/PLT/analysis_PLT/PLT/data/\ngdrive download 1omNcxLpV7KnHcWfHambx5R5_IqMvB8T2\n```\n\nLoad data:\n```{r}\ndBFA2_counts <- read.csv(here(\"data/dBFA2_counts_with_env_info.csv\"))\n```\n\nAdd `total.count` for each column, corresponding to each time point, rep, and env:\n```{r}\n# define columns containing time-point BC counts\ntime.cols <- grep('Time', names(dBFA2_counts), value = T)\n\n# generate long-form df:\nd2_c2 <- reshape2::melt(dBFA2_counts,\n                        value.name = 'count',\n                        variable.name = 'bfa.env.rep.time',\n                        measure.vars = time.cols)\n\n# parse bfa.env.time col into bfa.env, rep, and time:\nparse_names <- function(x){\n  unlist(strsplit(paste0(x), '\\\\.'))\n}\n\n# split names; re-name cols; store as new columns to working counts df:\nparsed_names_df <- data.frame(do.call(rbind, lapply(d2_c2$bfa.env.rep.time, parse_names)))[,-1]\nnames(parsed_names_df) <- c('bfa.env','bfa.rep','time')\nd2_c3 <- cbind(d2_c2, parsed_names_df)\n## this took way too long (~30 sec)... could make this faster with a series of matches. OH well.\n\n# add back count_totals, for logit slope calculation:\n# first calculate totals as colSums of time-point columns of original counts df\ncount_totals <- colSums(dBFA2_counts[,time.cols])\n\n# turn this into a mergable df\ndBFA2_totals_df <- data.frame(bfa.env.rep.time = names(count_totals), 'bfa.R' = as.vector(count_totals))\n\n# merge with focal data\nd2_c4 <- merge(d2_c3, dBFA2_totals_df, by = 'bfa.env.rep.time', sort = F)\n\n# change the way time is encoded from character string to numeric vector:\nd2_c4$time <- as.numeric(as.vector(gsub('Time','',d2_c4$time)))\n```\n\nLook at histogram of counts for the different sub-pools of the ancestral class:\n```{r, fig.height=4, fig.width=10, message=FALSE, warning=FALSE}\nd2_c4n <- d2_c4[d2_c4$Subpool.Environment == \"Ancestor_YPD_2N\",]\n\np1 <- ggplot(d2_c4n[(d2_c4n$bfa.env == 'FLC4') & (d2_c4n$time <= 16), ], aes(x = log(count,10), col = factor(time))) + \n  geom_density() + \n  facet_grid(Which.Subpools ~ bfa.rep) +\n  theme_bw() + ggtitle(\"FLC\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\np2 <- ggplot(d2_c4n[(d2_c4n$bfa.env == 'CLM') & (d2_c4n$time <= 16), ], aes(x = log(count,10), col = factor(time))) + \n  geom_density() + \n  facet_grid(Which.Subpools ~ bfa.rep) +\n  theme_bw() + ggtitle(\"CLM\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\nggarrange(plotlist = list(p1,p2), labels = c('a','b'), align = 'hv')\n```\n\nThere really is only one way to call fitness, which is if the starting frequency was sufficiently high to avoid hitting $r<10$ by $t_{16}$. We have very bad lower detection limits (scaled with frequency, yes).\n\nThe must heuristic and expedient way to correct for extinct barcodes is to add a single pseudo-count to all of the data, so that no barcode has `NA` fitness. This essentially enforces that all fitnesses estimated will be pinned at their lower bound, while noise in the non-zero count among the replicates will define the error in the measurements. For now let's only add the pseudocount to those BCs with $r_{t_{16}} = 0$ and then estimate the $\\text{logit}$ slopes.\n```{r}\n# pass data from FLC4 and CLM to function:\nd2_drug <- reshape2::dcast(d2_c4[d2_c4$bfa.env %in% c('FLC4','CLM'),],\n                                  #bfa.env.rep.time + \n                                    Full.BC + \n                                    #Total.Counts + \n                                    Subpool.Environment + \n                                    Which.Subpools + \n                                    bfa.env + \n                                    bfa.rep ~ time, value.var = 'count')\n\n# change names:\nnames(d2_drug)[match(c('8','16','24','32','40'), names(d2_drug))] <- c('r8','r16','r24','r32','r40')\n\nd2_drug_R <- reshape2::dcast(d2_c4[d2_c4$bfa.env %in% c('FLC4','CLM'),],\n                                  #bfa.env.rep.time + \n                                    Full.BC + \n                                    #Total.Counts + \n                                    Subpool.Environment + \n                                    Which.Subpools + \n                                    bfa.env + \n                                    bfa.rep ~ time, value.var = 'bfa.R')\n\n# change names:\nnames(d2_drug_R)[match(c('8','16','24','32','40'), names(d2_drug_R))] <- c('R8','R16','R24','R32','R40')\n\n# merge two dfs:\n# need to ensure that reshape has conducted itself identically between dfs so I can simply cbind them together..\n# should be all TRUE:\ntable(paste0(d2_drug$Full.BC,d2_drug$bfa.env,d2_drug$bfa.rep) == paste0(d2_drug_R$Full.BC,d2_drug_R$bfa.env,d2_drug_R$bfa.rep))\n\n# OK Yep. Just cbind() the dfs together:\nd2_drug <- cbind(d2_drug, d2_drug_R[,c('R8','R16','R24','R32','R40')])\n```\n\nHow many barcodes hit $r_{16} = 0$? \n```{r}\nlength(d2_drug[,'r16'][!d2_drug[,'r16'] > 0]) / length(d2_drug[,1])\n```\n\nThat's a huge proportion. Yikes. OK oh well! Let's go ahead and add the pseudo-count and take the logit slopes, calculated as\n$$s = \n\\dfrac{1}{T} \n\\times \\Bigg[ \n\\text{ln} \\Big( \\dfrac{f_t}{1-f_t} \\Big) - \\text{ln} \\Big( \\dfrac{f_0}{1-f_0} \\Big) \\Bigg]$$\n\nwhere $T$ is the elapsed number of generations.\n```{r}\n# add pseudo-count\n# we will only do this for t16, since those with too few counts at t8 have already been excluded\nd2_drug[,'r16'][!d2_drug[,'r16'] > 0] <- 1\n\n# def logit slope function\ncalc_logit_slopes <- function(x, t1='r8', t2='r16', R1 = 'R8', R2 = 'R16', TT = 8){\n  f0 <- x[,t1] / x[,R1]\n  f1 <- x[,t2] / x[,R2]\n  s <- (1/TT) * ( log( f1 / (1 - f1) ) - log( f0 / (1 - f0) ) )\n  return(s)\n}\n\n# run script to calculate for all rows:\nd2_drug$s_est <- calc_logit_slopes(d2_drug)\n```\n\nLet's give this a plot, specifically for the neutral class:\n```{r, fig.height=3.5, fig.width=5, message=FALSE, warning=FALSE}\ndp1 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'Ancestor_YPD_2N',], aes(x = s_est, col = bfa.rep)) +\n  geom_density() + \n  facet_grid(bfa.env ~ Subpool.Environment) + \n  theme_bw()\n\ndp1\n```\n\nWe can see that the zero-counts which received the +1 pseudocount typically exist in a lower mode of fitness estimates. I'm not sure of any reasonable way to up-shrink these estimates. I imagined that the entire distribution would be unimodal, but I was wrong. This probably has to do with the very large noise properties in these assays. Additionally, the three replicates are different from one another in quite dramatic ways, especially for `FLC4`: I should basiclaly toss out `R3` since the read counts are so low.\n\nThis is a problem, so much so that we may have to re-do this BFA so we get more reasonable coverage and less extreme swings in frequency. We can discuss this on the phone on Wednesday.\n\nBefore we move on, let's take a look at the other source environments, specifically `FLC4` (a) and `CLM` (b) home environments:\n```{r, fig.height=2.5, fig.width=7, message=FALSE, warning=FALSE}\ndp2 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'FLC4_2N',], aes(x = s_est, col = bfa.rep)) + \n  geom_density() + \n  facet_wrap( ~ bfa.env) +\n  ggtitle('FLC4')\n\ndp3 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'CLM_2N',], aes(x = s_est, col = bfa.rep)) + \n  geom_density() + \n  facet_wrap( ~ bfa.env) + \n  ggtitle('CLM')\n\nggarrange(plotlist = list(dp2,dp3), labels = c('a','b'), align = 'hv', common.legend = T)\n```\n\nThese have not been corrected for neutral fitness, so you can't compare location of distributions to get information about distributions of fitness effects.\n\nWhat's next?\n\n1. Organize dBFA2 data for all relevant environments, subtracting off the crazy drug stuff\n2. Do the same with hBFA1 data\n3. determine adapted for all environments\n \nGoal is to have some kind of dendrogram figure to compile with some other JDFE-style figures to show folks tomorrow.\n\n## *prima facie* analysis of fitness patterns\n\nLet's pretend we don't care about the incredibly wonky fitness patterns produced in the drug environemnts. We'll simply take the complete sub-set of the data in order to generate mock-up plots of what we might want our analyses to look like. First up depends on us determining adapted sub-set and clustering them based on their fitness traces.\n\nLet's first start by calling adapted. For `dBFA2` we will ignore the drug environments and then simply take the $s$ estimates for these environments as given for all the lineages defined as adapted on the basis of all the other environments. To do this, for all lineages for all bfa.env, we calculate the empirical quantile of estimated fitness in the distribution of neutral class estimates. We do the same for the neutrals, which constitutes our testing distribution. We will then estimate some form of cutoff for which false discovery rate is empirically adjusted to ~5% across the dataset. We could go lower, too.\n\nUse `data.frame` `d4` for all of this downstream stuff.\n```{r}\n# for all lineages for all bfa.env, calculate empirical quantile of estimated fitness in the distribution of neutral class estimates.\n# fastest way will be to derive testing distribution for each bfa.env combination.. excluding CLM and FLC4\n\n# define bfa.env set I want to look through\nenvs <- unique(d4$bfa.env)\nenvs <- envs[!envs %in% c('FLC4','CLM')]\n\n# pass vector of barcodes as neutral set\nneuts <- unique(d4[d4$Subpool.Environment == 'Ancestor_YPD_2N','Full.BC'])\n\n# define function to do loop, so I can recylce it for each BFA\ncalc_neut_t <- function(x, ENVS, NEUTS){\n  # define empty results df\n  res <- data.frame()\n  \n  # loop through environments\n  for (e in 1:length(ENVS)){\n    # extract neutral lineages\n    neut_BCs <- x[(x$Full.BC %in% NEUTS) & (x$bfa.env == ENVS[e]),]\n    \n    # remove any NA or any Inf:\n    neut_BCs <- neut_BCs[!neut_BCs$iva.s == Inf,]\n    \n    # calculate weighted average and weighted sigma for neutral fitness:\n    # fist, define weights.\n    # do so proportional to 1/sigma, standardized by sum\n    w <- 1/neut_BCs$iva.s_err\n    \n    # calculate sum of weights:\n    sw <- sum(1/neut_BCs$iva.s_err)\n    \n    # calculate (ML) weighted sum, standardized to sum of weights:\n    w_mu <- sum(neut_BCs$iva.s * w) / sw \n    \n    # calculate (ML) weighted standard deviation of this sample\n    w_sd <- sqrt(sum((w/sw) * (neut_BCs$iva.s - w_mu)^2))\n    \n    # return the results for the relevant bfa.env:\n    res <- rbind(res, data.frame(bfa.env = ENVS[e], w_mu = w_mu, w_sd = w_sd))\n  }\n  \n  # merge back to main df, matching by bfa.env\n  x2 <- merge(x, res, by = 'bfa.env', sort = F, all.x = T)\n  \n  # calculate test statistic for all barcodes:\n  x2$t_star  <- (x2$iva.s - x2$w_mu) / (x2$iva.s_err + x2$w_sd)\n  \n  # output the modified df\n  return(x2)\n}\n\nd5 <- calc_neut_t(d4, ENVS = envs, NEUTS = neuts)\n```\n\nLet's look into excluding any barcodes with super wonky errors (compared to their estimates):\n```{r, fig.height=2, fig.width=6, message=FALSE, warning=FALSE}\nd4$s_s_err <- abs(d4$iva.s_err)\n\nd4b <- dplyr::group_by(d4[!d4$bfa.env %in% c('FLC4','CLM'),], Full.BC) %>% summarise(mean_s_s_err = mean(s_s_err, na.rm = T), err_max = max(s_s_err, na.rm = T), err_min = min(s_s_err, na.rm = T)) %>% arrange(mean_s_s_err)\n\nd4b$Full.BC <- factor(d4b$Full.BC, levels = unique(paste0(d4b$Full.BC)))\nd4b$neutral <- 0\nd4b$neutral[d4b$Full.BC %in% neuts] <- 1\n\nggplot(d4b, aes(x = Full.BC, y = mean_s_s_err, ymax = err_max, ymin = err_min)) + geom_linerange(col = \"gray80\") +\n  geom_point(col = \"gray40\", size = 0.75) + facet_wrap(~ neutral) +\n  theme(axis.text.x = element_blank(),\n        axis.line.x = element_blank(),\n        axis.ticks.x = element_blank()) + xlab(\"\")\n```\n\nThere aren't any neutrals with wonky errors (perhaps a couple; these will throw off the critical value estimation, below..). Wouldn't be a bad idea to cut out this top 1% that looks terrible.\n\nAnyhow.\n\nLet's make some plots of the t-scores, especially of the neutral lineages themselves. We can iterate and discard any wonky looking guys, else include them as part of the false discovery set.\n```{r, fig.height=3.5, fig.width=5, message=FALSE, warning=FALSE}\nggplot(d5[d5$Full.BC %in% paste0(neuts),], aes(x = t_star)) + geom_density() + facet_wrap(~ bfa.env, scales = 'free')\n```\n\nThese look like fairly reasonable testing distributions. Let's plot the distribution of the sum of the abs(t-scores):\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\n# need to exclude FLC4 and CLM for these calculations:\nd5b <- d5[!d5$bfa.env %in% c('FLC4','CLM'),]\ntsums <- dplyr::group_by(d5b, Full.BC) %>% summarise(t_chi = sum(t_star^2), n_envs = length(t_star)) %>% mutate(t_chi_avg = t_chi / n_envs)\n\nt_sum_dist_p <- ggplot(tsums[tsums$Full.BC %in% paste0(neuts),], aes(x = t_chi)) + geom_density() + theme_bw() + ggtitle(\"neutral t_Chisq dist\") +\n  scale_x_continuous(limits = c(0,20))\nt_sum_dist_p\n```\n\nLet's fit (using ML) the $df$ parameter of the $\\chi^2$ distribution to these data and inspect the fit:\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\n# fit Chi-squared distribution to this, truncating at the upper part (i.e. >10):\nchis <- as.data.frame((tsums[tsums$Full.BC %in% paste0(neuts),'t_chi']))\nemp_df <- fitdistr(chis$t_chi[chis$t_chi != Inf], \"chi-squared\", start = list(df = 1))\n\n# make vector of densities for Chisq with df = emp_df$estimate\nchis_dist <- data.frame(dchi = dchisq(seq(0,20,0.01), df = emp_df$estimate), est = seq(0,20,0.01))\n\n# plot as overlap on previous graph:\nt_sum_dist_p <- t_sum_dist_p + geom_line(data = chis_dist, aes(x = est, y = dchi), col = \"darkorange\", lty = \"dashed\") \nt_sum_dist_p\n```\n\nSo, the fit isn't perfect, but it looks like we have something approximating a $\\chi^2$ distribution for our neutral barcodes. We might as well use the empirical quantile just so we don't accidentally accept barcodes as adapted if their `t_chi` is close to zero (the point at which the densitites in the plot diverge maximally). So, we find the quantile which gives a FDR of 5%:\n\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\nfdr <- quantile(chis$t_chi, probs = c(0.95))\n#fdr <- cbind(sort(chis$t_chi),seq(1:length(chis$t_chi))/length(chis$t_chi))\n\n# find the 95% quantile of the Chisq with optimized df:\nchi95 <- qchisq(0.95, df = emp_df$estimate)\n\nt_sum_dist_p <- t_sum_dist_p + geom_vline(xintercept = fdr, col = \"midnightblue\") + geom_vline(xintercept = chi95, col = \"magenta\")\nt_sum_dist_p\n```\nWhere the magenta cutoff is the theoretical critical value and the darkblue line is the empirical one.\n\nIf we removed the bizarre-looking outliers we would probably converge back to the $\\chi^2$ critical value corresponding to $\\alpha = 0.05$. Our empirical cutoff is substantially higher. For now, however, we will use out empirical cutoff in order to call lineages as adaptive or not:\n```{r, fig.height=2, fig.width=10, message=FALSE, warning=FALSE}\ntsums$adapted <- 0\ntsums$adapted[tsums$t_chi >= fdr] <- 1\n\ntsums$neutral <- 0\ntsums$neutral[tsums$Full.BC %in% paste0(neuts)] <- 1\n\n# bring back in source.env information:\ntsum2 <- merge(tsums, unique(d5[,c('Full.BC','Subpool.Environment')]), by = 'Full.BC', sort = F)\n\n# exclude spurious environment calls:\ntsum2 <- tsum2[!tsum2$Subpool.Environment %in% c('none','not_read'),]\n# write as csv for now:\nwrite.csv(tsum2, file = here(\"data/dBFA2_adapted_calls_20-APR-2018.csv\"), quote = F, row.names = F)\n\n# plot these sums by bfa.env and source env\nggplot(tsum2, aes(x = factor(adapted), y = log(t_chi), col = factor(adapted))) + \n  geom_jitter(width = 0.1, alpha = 0.1, aes(col = factor(adapted))) +\n  geom_boxplot(alpha = 0.5) + \n  facet_wrap(~ Subpool.Environment, scales = 'free', ncol = 12) +\n  theme_bw() + scale_color_manual(values = c('gray60','magenta2')) + \n  theme(legend.position = 'none', strip.background = element_blank()) +\n  xlab('adapted') + ylab('log(chisq)')\n```\n\nThis seems like a perfectly reasonable way to get things moving. Clearly there is some threshold that is continuously met at the cutoff, so we may decide to implement some kind of Bayesian classifier that optimizes both positive and negative predictive value of the classifier. We'll leave that for a future iteration of the analysis.\n\nLet's count up the adapted clones from each source environment:\n```{r}\naBCs <- tsums$Full.BC[tsums$adapted==1]\nd5$adapted <- 0\nd5$adapted[d5$Full.BC %in% aBCs] <- 1\nd5$neutral <- 0\nd5$neutral[d5$Full.BC %in% paste0(neuts)] <- 1\n\n# remove spurious BCs:\nd5 <- droplevels(d5[!d5$Subpool.Environment %in% c('none','not_read'),])\nt1 <- table(d5$adapted, d5$Subpool.Environment)\nt1\n```\n\nPlot this:\n```{r, fig.height=3, fig.width=4, message=FALSE, warning=FALSE}\nt3 <- reshape2::melt(t(data.frame(apply(t(t1),1,function(x) x/sum(x))))) %>% dplyr::arrange(desc(Var2), desc(value))\nnames(t3) <- c('source','adapted','prop.adapt')\nt3$source <- factor(t3$source, levels = paste0(t3$source))\n\n# re-sort t3 based on prop adaptive:\nprop_adapt_p <- ggplot(t3, aes(x = source, y = prop.adapt, fill = factor(adapted))) + \n  geom_bar(stat = 'identity') + scale_fill_manual(values = c('gray90','dodgerblue')) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\n  geom_hline(yintercept = 0.05, lty = 'dashed', col = 'darkorange')\nprop_adapt_p\n```\nDashed line is 5%, and the neutral class is right where it should be (with 5% called as adapted).\n\nLet's plot the spagetti plot for the relevant source environments. We'll exclude 48hr, and we'll also exclude the 37C at Stanford `bfa.env`:\n\n```{r}\nd6 <- droplevels(d5[d5$bfa.env != 'X37C_Stan',])\nd6 <- droplevels(d6[d6$Subpool.Environment != '48Hr_2N',])\nggplot(d6, aes(x = bfa.env, y = iva.s, group = Full.BC, col = Subpool.Environment)) + \n  geom_line(alpha = 0.4) + \n  #theme_bw() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\n  facet_wrap(~ Subpool.Environment)\n```\n\n**Next steps include:**\n  1. building dendrogram of these data\n  2. bringing in hBFA1 data. Should be reasonably fast.\n  3. Plotting with source env. and ploidy as factors. Sub-sampling the clusters for clarity.\n  4. Calculating summary pleiotropy statistics for the clusters.\n  \n## Producing heatmaps\n\n*What sort of meta-data do I want to plot?*\n\n1. Source environment\n2. Ploidy\n\nFor now I can quickly bring over some haploid data just for the figure mock-up. Let's maybe quickly try to bring in hBFA1 data, actually:\n\n## hBFA1 data parsing\n\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\n### hBFA1 data parsing routine:\n# grab columns corresponding to inverse variance weighted (by rep) average fitness estimates\nh1cols <- grep('iva_s', names(hBFA1), value = T)\n\n# grab cols corresponding to the variance of these estimates\nh1cols_err <- grep('_err', h1cols, value = T)\n\nmeta_cols <- names(hBFA1)[1:5] # define meta-data for BCs to capture\nhBFA1_2     <- hBFA1[,(names(hBFA1) %in% c(meta_cols, h1cols)) & (!names(hBFA1) %in% h1cols_err)]\nhBFA1_2_err <- hBFA1[,names(hBFA1) %in% c(meta_cols, h1cols_err)]\n\n# re-name columns:\nh1cols2 <- gsub('.iva_s','', h1cols)\nh3 <- reshape2::melt(hBFA1_2, value.name = 'iva.s', variable.name = 'bfa.env')\n\n# bring over the error col as well\nh3_err <- reshape2::melt(hBFA1_2_err, value.name = 'iva.s_err', variable.name = 'bfa.env')\n\n# re-name bfa.env without 'iva_s'\nh3$bfa.env <- gsub('\\\\.iva_s', '', h3$bfa.env)\nh3_err$bfa.env <- gsub('\\\\.iva_s_err', '', h3_err$bfa.env)\n\n# merge the two:\nh4 <- merge(h3,h3_err,sort = F)\n\n# change how iva.s and iva.s_err are reported, from per-cycle to per-generation:\nh4$iva.s <- h4$iva.s / 8\nh4$iva.s_err <- h4$iva.s_err / 8\n\n# define bfa.env set I want to look through\nhenvs <- unique(h4$bfa.env)\nhenvs <- henvs[!henvs == 'X48Hr'] # remove 48Hr environment\n\n# pass vector of barcodes as neutral set\nhneuts <- unique(h4[h4$Subpool.Environment == 'YPD_alpha','Full.BC'])\n\n# calc t-score relative to neutrals\nh5 <- calc_neut_t(h4, ENVS = henvs, NEUTS = hneuts)\n\n# plot sum of squared t-scores:\n#h5b <- h5[!h5$bfa.env %in% c('FLC4','CLM'),]\n\nhtsums <- dplyr::group_by(h5, Full.BC) %>% summarise(t_chi = sum(t_star^2, na.rm = T), n_envs = length(t_star)) %>% mutate(t_chi_avg = t_chi / n_envs)\n\nht_sum_dist_p <- ggplot(htsums[htsums$Full.BC %in% paste0(hneuts),], aes(x = t_chi)) + geom_density() + theme_bw() + ggtitle(\"neutral t_Chisq dist\") +\n  scale_x_continuous(limits = c(0,5))\nht_sum_dist_p\n```\n\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\nchis <- as.data.frame((htsums[htsums$Full.BC %in% paste0(hneuts),'t_chi']))\nemp_df <- fitdistr(chis$t_chi[chis$t_chi != Inf], \"chi-squared\", start = list(df = 1))\n\n# make vector of densities for Chisq with df = emp_df$estimate\nchis_dist <- data.frame(dchi = dchisq(seq(0,20,0.01), df = emp_df$estimate), est = seq(0,20,0.01))\n\n# plot as overlap on previous graph:\nht_sum_dist_p <- ht_sum_dist_p + geom_line(data = chis_dist, aes(x = est, y = dchi), col = \"darkorange\", lty = \"dashed\") \nht_sum_dist_p\n```\n\n```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}\n# this fit is way worse that for dBFA2. Not sure what's up. Maybe fit a non-centrality parameter?\n\nfdr <- quantile(chis$t_chi, probs = c(0.95))\n#fdr <- cbind(sort(chis$t_chi),seq(1:length(chis$t_chi))/length(chis$t_chi))\n\n# find the 95% quantile of the Chisq with optimized df:\nchi95 <- qchisq(0.95, df = emp_df$estimate)\n\nht_sum_dist_p <- ht_sum_dist_p + geom_vline(xintercept = fdr, col = \"midnightblue\") + geom_vline(xintercept = chi95, col = \"magenta\")\nht_sum_dist_p\n```\n\nThe dark blue line (empirical cutoff) is less conservative than the $\\chi^2$ 95% critical value, but since we see that it fits so poorly, we'll stick to the empirical cutoff here, as we did for dBFA2 above.\n```{r}\nhtsums$adapted <- 0\nhtsums$adapted[htsums$t_chi >= fdr] <- 1\n\nhtsums$neutral <- 0\nhtsums$neutral[htsums$Full.BC %in% paste0(hneuts)] <- 1\n\n# bring back in source.env information:\nhtsum2 <- merge(htsums, unique(h5[,c('Full.BC','Subpool.Environment')]), by = 'Full.BC', sort = F)\n\n# exclude spurious environment calls:\nhtsum2 <- htsum2[!htsum2$Subpool.Environment %in% c('none','not_read'),]\n\nhaBCs <- htsums$Full.BC[htsums$adapted==1]\nh5$adapted <- 0\nh5$adapted[h5$Full.BC %in% haBCs] <- 1\nh5$neutral <- 0\nh5$neutral[h5$Full.BC %in% paste0(hneuts)] <- 1\n\n# remove spurious BCs:\nh5 <- droplevels(h5[!h5$Subpool.Environment %in% c('none','not_read'),])\nht1 <- table(h5$adapted, h5$Subpool.Environment)\nht1\n```\n\n```{r, fig.height=3, fig.width=4, message=FALSE, warning=FALSE}\nht3 <- reshape2::melt(t(data.frame(apply(t(ht1),1,function(x) x/sum(x))))) %>% dplyr::arrange(desc(Var2), desc(value))\nnames(ht3) <- c('source','adapted','prop.adapt')\nht3$source <- factor(ht3$source, levels = paste0(ht3$source))\n\n# re-sort t3 based on prop adaptive:\nhprop_adapt_p <- ggplot(ht3, aes(x = source, y = prop.adapt, fill = factor(adapted))) + \n  geom_bar(stat = 'identity') + scale_fill_manual(values = c('gray90','dodgerblue')) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +\n  geom_hline(yintercept = 0.05, lty = 'dashed', col = 'darkorange')\nhprop_adapt_p\n```\n\nSpit out figure for dBFA2 and hBFA1 side by side:\n\n```{r, fig.height=3, fig.width=5, message=FALSE, warning=FALSE}\nggarrange(plotlist = list(prop_adapt_p,hprop_adapt_p), labels = c('a','b'), align = 'hv', common.legend = T,\n          widths = c(1,0.825))\n```\n\nOK not bad! These haven't been corrected for diploids though.\n\n### Exporting working dBFA2 fitness file\n\n```{r}\n# split dh5 into two: one with meta-data, the other with fitnesses, then re-join:\nd5$sources <- gsub('_2N','',d5$Subpool.Environment)\n\n# now define ploidy state\nd5$ploidy <- '2N'\n\n# flag if auto-diploid:\nd5$subpool <- 'R2'\nd5$subpool[grep('-R1',d5$Which.Subpools)] <- 'R1'\n\nd5_meta <- unique(d5[,names(d5) %in% c('Full.BC','adapted','neutral','source','ploidy','subpool')])\nd5_2 <- d5[,names(d5) %in% c('Full.BC','sources','bfa.env','iva.s')]\n\n# currently this call isn't working:\nd5_long <- reshape2::dcast(d5_2, Full.BC + sources ~ bfa.env, value.var = 'iva.s', fun.aggregate = mean)\n\n# add back meta-data:\nd5_long2 <- merge(d5_long,d5_meta, by = 'Full.BC', sort = F)\n\nwrite.csv(d5_long2, file = here('data/dBFA2_fitnesses_with_adapt_20APR2018.csv'), quote = F)\n```\n\n\n## Putting together a joint dendrogram from both assays\n\nWe start by combining the adapted BCs from dBFA2 and hBFA1 into a common df. We then cluster and plot as a dendrogram, with ploidy as a factor, as well as source environment. We may choose to down-sample barcodes belonging to the same clusters, which we will define using `cuttree` once we have the clust object to work tith.\n```{r}\n# combining dfs:\n# using d5 and h5 as input dfs:\ndh5 <- rbind(d5[d5$adapted==1,],h5[h5$adapted==1,])\n\n# Let's add back a handul of neutral guys from both dBFA2 and hBFA1\nd5n <- d5[d5$neutral==1,]\nh5n <- h5[h5$neutral==1,]\n\n# sample neutral BCs from each BFA:\nns <- 100\nd.samp <- sample(unique(d5n$Full.BC),ns, replace = F)\nh.samp <- sample(unique(h5n$Full.BC),ns, replace = F)\n\nd5n2 <- d5n[d5n$Full.BC %in% d.samp,]\nh5n2 <- h5n[h5n$Full.BC %in% h.samp,]\n\n# add back to dh5\"\ndh5 <- rbind(dh5,rbind(d5n2,h5n2))\n\n# split Subpool.Environment into source and ploidy:\nsources <- gsub('_2N','',dh5$Subpool.Environment)\nsources <- gsub('_alpha','',sources)\n# add it back in\ndh5$source <- sources\n\n# now define ploidy state\ndh5$ploidy <- '2N'\ndh5$ploidy[grep('alpha',dh5$Subpool.Environment)] <-'1N'\n\n# flag if auto-diploid:\ndh5$subpool <- 'R2'\ndh5$subpool[grep('-R1',dh5$Which.Subpools)] <- 'R1'\n\ndh5$autodip <- 0\ndh5$autodip[grep('autodiploids',dh5$Which.Subpools)] <- 1\n\n# write this to file:\nwrite.csv(dh5, file = here('data/dBFA2_hBFA1_with_adapt_11APR2018.csv'), quote = F)\n```\n\nNow we're ready to plot a dendrogram. We need to convert our joint df into wide-format with `dcast`:\n```{r}\n# remove 48Hr from source and bfa.env\ndh5 <- dh5[dh5$source != '48Hr',]\ndh5 <- dh5[dh5$bfa.env != 'X48Hr',]\ndh5 <- dh5[dh5$bfa.env != 'X37C_Stan',]\n\ndh5$source <- factor(dh5$source)\ndh5$bfa.env <- factor(dh5$bfa.env)\n\n# split dh5 into two: one with meta-data, the other with fitnesses, then re-join:\ndh5_meta <- unique(dh5[,names(dh5) %in% c('Full.BC','adapted','neutral','source','ploidy','subpool','autodip')])\ndh6 <- dh5[,names(dh5) %in% c('Full.BC','bfa.env','iva.s')]\n\n# currently this call isn't working:\ndh7 <- reshape2::dcast(dh6, Full.BC ~ bfa.env, value.var = 'iva.s', fun.aggregate = mean)\n\n# add back meta-data:\ndh8 <- merge(dh7,dh5_meta, by = 'Full.BC', sort = F)\nrow.names(dh8) <- dh8$Full.BC\n\n# bfa.env names:\nbfa.envs <- names(dh8)[2:11]\n\n# write dh8 to file for convenience:\nwrite.csv(dh8, here(\"data/dBFA2_hBFA1_fitnesses_allBCs.csv\"), row.names = F, quote = F)\n```\n\n\n<!-- ```{r} -->\n<!-- # ncol <- 20 -->\n<!-- # cols <- RColorBrewer:::brewer.pal(11,\"PuOr\") -->\n<!-- # rampcols <- colorRampPalette(colors = cols, space=\"Lab\")(ncol) -->\n<!-- # rampbreaks <- seq(0, 100, length.out = ncol+1) -->\n\n<!-- ## more stuff leading up to dendro: -->\n<!-- # define custom ramp: -->\n<!-- nn <- 16 -->\n<!-- rc1 = colorRampPalette(colors = c(\"#f1a340\", \"white\"), space=\"Lab\")(nn) -->\n<!-- ## Make vector of colors for values above threshold -->\n<!-- rc2 = colorRampPalette(colors = c(\"white\", \"#998ec3\"), space=\"Lab\")(nn) -->\n<!-- rampcols = c(rc1, rc2) -->\n<!-- ## In your example, this line sets the color for values between 49 and 51. -->\n<!-- rampcols[c(nn, nn+1)] = rgb(t(col2rgb(\"white\")), maxColorValue=256) -->\n\n<!-- # Min = min(Jall_new3) -->\n<!-- # Max = max(Jall_new3) -->\n<!-- Min = -0.3 -->\n<!-- Max = 0.3 -->\n<!-- Thresh = 0 -->\n\n<!-- rb1 = seq(Min, Thresh, length.out=nn+1) -->\n<!-- rb2 = seq(Thresh, Max, length.out=nn+1)[-1] -->\n<!-- rampbreaks = c(rb1, rb2) -->\n<!-- ``` -->\n\nInstall `heatmap.3` function:\n```{r}\ninstall.packages(\"devtools\")\nlibrary(\"devtools\")\nsource_url(\"https://raw.githubusercontent.com/obigriffith/biostar-tutorials/master/Heatmaps/heatmap.3.R\")\n```\n\n<!-- ```{r} -->\n<!-- random_rows <- sample(seq(1,length(dh8[,1]),1),size = 250, replace=F) -->\n<!-- test_input <- dh8[random_rows,bfa.envs] -->\n<!-- test_input <- test_input[complete.cases(test_input),] -->\n\n<!-- test_meta <- dh5_meta[dh5_meta$Full.BC %in% test_input$Full.BC,] -->\n<!-- #row.names(test_meta) <- test_meta$Full.BC -->\n<!-- #test_meta <- test_meta[,-1] -->\n\n<!-- test_meta$source <- factor(test_meta$source) -->\n<!-- test_meta$ploidy <- factor(test_meta$ploidy) -->\n\n<!-- # need to color all of the meta-data: -->\n<!-- #library(viridis) -->\n<!-- scolors <- -->\n<!--    with(test_meta, -->\n<!--         data.frame(source = levels(source), -->\n<!--                    scolor = I(viridis(nlevels(source))))) -->\n\n<!-- pcolors <- -->\n<!--    with(test_meta, -->\n<!--         data.frame(ploidy = levels(ploidy), -->\n<!--                    pcolor = I(magma(nlevels(ploidy))))) -->\n\n\n<!-- test_meta <- merge(test_meta, scolors, by = 'source', sort = F) -->\n<!-- test_meta <- merge(test_meta, pcolors, by = 'ploidy', sort = F) -->\n<!-- #test_meta$source <- relevel(test_meta$source, levels = sample(viridis(12), length(unique(test_meta$source)))) -->\n<!-- row.names(test_meta) <- test_meta$Full.BC -->\n<!-- test_meta <- test_meta[,-1] -->\n\n<!-- rowcolz <- rbind(paste0(test_meta$scolor),paste0(test_meta$pcolor)) -->\n<!-- ``` -->\n\nHeatmap-specific functions:\n```{r}\nInf_to_NA <- function(x){\n  # find rows with Inf:\n  inf.rows <- apply(x[,-1],1,function(x) sum(is.infinite(x)))\n  \n  # find submatrix with Infs:\n  sm <- x[inf.rows>0 , ]\n  \n  # traverse rows\n  for (r in 1:length(sm[,1])){\n    sm[r,-1][is.infinite(as.numeric(as.vector(sm[r,-1])))] <- NA\n  }\n  \n  # substitute Inf-->NA rows back into input df\n  x[inf.rows>0,] <- sm\n  \n  # output modified input df\n  return(x)\n}\n\n# define custom dist and clust functions for use with heatmap.3\n#mydist=function(c) {daisy(Inf_to_NA(c))}\nmydist=function(c) {dist(c, method = 'euclidean')}\nmyclust=function(c) {hclust(c, method=\"average\")}\n```\n\n\nLet's make a heatmap, with all of the lineages:\n```{r}\n#random_rows <- sample(seq(1,length(dh8[,1]),1),size = 250, replace=F)\nbfa.envs <- bfa.envs[!bfa.envs %in% c('X02M_NaCl','X37C_Stan')]\ntest_input <- dh8[,bfa.envs]\n\n# turn all Inf values to NA\ntest_input <- Inf_to_NA(test_input)\n\n# remove all BCs with >1 NA\nNAs <- apply(test_input[,-1], 1, function(x) sum(is.na(x)))\ntest_input <- test_input[!NAs>1,]\n\n# FOR NOW:\n# turn all NAs into zeros so I can actually get through this heatmap plotting\ntest_input[is.na(test_input)] <- 0\n```\n\nLet's do some scaling. For visual display, I'd like to plot all of the data on the same relative scale, standardized to between min and max, and mean-centered.\nI will also include meta-data for the bfa.envs, where I plot the mean and variance (i.e., standard deviation) of fitness effects (untransformed) as a single dimension heatmap.\n\n```{r}\n# re-scaling:\n# x <- test_input[,bfa.envs[4]]\n# mx <- mean(x)\n# minx <- min(x)\n# maxx <- max(x)\n# sdx <- sd(x)\n\n### obsolete:\n# re_scale <- function(x){\n  #   rank(x)/length(x)\n  #   (x - mean(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T))  \n  # }\n\n# function to re-scale by quantiles for positive and negative range separately:\n# (signed quantile rescaling)\nsqr_by_env <- function(x){\n  # define function to calculate signed quantile rescaling\n  sqr <- function(x){\n    x2 <- x>0 #take positive\n    x3 <- x<0 #take negatives\n    # re-scale positives\n    x[x2] <- rank(x[x2]) / length(x[x2])\n    x[x3] <- (-1) * rank(abs(x[x3])) / length(x[x3])\n    return(x)\n  }\n  \n  # apply function over all input columns separately\n  x2 <- do.call(cbind, lapply(x, sqr))\n  \n  # return transformed \n  return(x2)\n}\n\n# try out:\ntest_in2 <- sqr_by_env(test_input[, bfa.envs[-1]])\n```\n\n\nGenerate plots to correspond the rank to the actual fitness values:\n```{r, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}\ntest_in3 <- cbind(reshape2::melt(test_in2, value.name = 'quantile'),\n                  reshape2::melt(test_input[, bfa.envs[-1]], value.name = 'fitness'))\n\nggplot(test_in3, aes(x = fitness, y = quantile)) + geom_line() + facet_wrap(~ Var2) + \n  geom_vline(xintercept = 0, col = 'gray60', lty = 'dotted') +\n  geom_hline(yintercept = 0, col = 'gray60', lty = 'dotted')\n```\n\nThis plot shows the relationship between signed quantile and actual fitness. We will be plotting quantiles in the heatmap, each of which corresponds to a different extent of variation both above and below ancestral fitness.\n\n```{r}\n# capture resulting BCs and grab their meta-data:\n# test_meta <- dh8_meta[dh5_meta$Full.BC %in% test_input$Full.BC,]\nmeta_cols <- c('source','ploidy','subpool','autodip','neutral')\ntest_meta <- droplevels(dh8[dh8$Full.BC %in% test_input$Full.BC, meta_cols])\n\n#### define colors ####\ntest_meta$source <- factor(test_meta$source)\ntest_meta$subpool <- factor(test_meta$subpool)\ntest_meta$ploidy <- factor(test_meta$ploidy)\ntest_meta$autodip <- factor(test_meta$autodip)\ntest_meta$neutral <- factor(test_meta$neutral)\n\n# define source colors (12-level):\nscolors <-\n   with(test_meta,\n        data.frame(source = levels(source),\n                   scolor = RColorBrewer::brewer.pal(length(levels(source)), \"Paired\")))\n\n# define ploidy colors (two-level):\npcolors <-\n   with(test_meta,\n        data.frame(ploidy = levels(ploidy),\n                   pcolor = c('#deebf7','#3182bd')))\n\n# define neutral colors (two-level):\nncolors <- with(test_meta,\n        data.frame(neutral = levels(neutral),\n                   ncolor = c('#f0f0f0','#636363')))\n\n# define autodiploid colors (two-level):\nacolors <- with(test_meta,\n        data.frame(autodip = levels(autodip),\n                   acolor = c('#efedf5','#de2d26')))\n\n# add back to meta data:\ntest_meta <- merge(test_meta, scolors, by = 'source', sort = F)\ntest_meta <- merge(test_meta, pcolors, by = 'ploidy', sort = F)\ntest_meta <- merge(test_meta, acolors, by = 'autodip', sort = F)\ntest_meta <- merge(test_meta, ncolors, by = 'neutral', sort = F)\n\n#test_meta$source <- relevel(test_meta$source, levels = sample(viridis(12), length(unique(test_meta$source))))\nrow.names(test_meta) <- test_meta$Full.BC\ntest_meta <- test_meta[,-1]\n\nrowcolz <- rbind(paste0(test_meta$scolor),\n                 #paste0(rep(\"white\", length(test_meta$scolor))), # add whitespace col between factor\n                 paste0(test_meta$pcolor),\n                 #paste0(rep(\"white\", length(test_meta$scolor))), # add whitespace col between factor\n                 paste0(test_meta$acolor),\n                 #paste0(rep(\"white\", length(test_meta$scolor))), # add whitespace col between factor\n                 paste0(test_meta$ncolor))\n```\n\nBelow are parameters for heatmap colors:\n```{r}\n# nn <- 6\n# rc1 = colorRampPalette(colors = c(\"#543005\", \"#f5f5f5\"), space=\"Lab\")(nn)\n# #rc1 = colorRampPalette(colors = c(\"magenta\", \"white\"), space=\"Lab\")(nn)\n# ## Make vector of colors for values above threshold\n# rc2 = colorRampPalette(colors = c(\"#f5f5f5\", \"#003c30\"), space=\"Lab\")(nn)\n# rampcols = c(rc1, rc2[-1])\n\n#rampcols <- RColorBrewer::brewer.pal(11,name = \"BrBG\")\nrampcols <- viridis::viridis(20)\n\nrampcols <- c(\"#7f3b08\",\"#b35806\",\"#e08214\",\"#fdb863\",\"#fee0b6\",\"#f7f7f7\",\"#d8daeb\",\"#b2abd2\",\"#8073ac\",\"#542788\",\"#2d004b\")\n\n# Min = min(Jall_new3)\n# Max = max(Jall_new3)\nMin = -1\nMax = 1\nThresh = 0\n# rb1 = seq(Min, Thresh, length.out=nn)\n# rb2 = seq(Thresh, Max, length.out=nn)[-1]\n# rampbreaks = c(rb1, rb2)\n\nrampbreaks = seq(-1,1,0.1)\n```\n\nTry to produce ploidy-source composite factor with dummy variables and color mapping:\n```{r}\ntest_meta$source_ploidy <- paste0(test_meta$source,'_',test_meta$ploidy)\ntest_meta$ID <- seq(1:length(test_meta[,1]))\ntest_meta2 <- cbind(test_meta, mm1 <- model.matrix(ID ~ source, data = test_meta)[,-1])\n\n# define color scales:\ncolorz <- data.frame(matrix(c(\"#9ecae1\",\"#3182bd\",\"#a1d99b\",\"#31a354\",\"#fdae6b\",\"#e6550d\",\"#bcbddc\",\"#756bb1\",\"#fc9272\",\"#de2d26\",\"#fa9fb5\",\"#c51b8a\",\"#c994c7\",\"#dd1c77\",\"#8c96c6\",\"#88419d\",\"#fdcc8a\",\"#fc8d59\"), ncol = 9))\nnames(colorz) <- bfa.envs[-1]\n\n# iterate through mm1 names and flag each \n\n\n```\n\n\n\nProduce new heatmap:\n```{r, fig.height=16, fig.width=6, message=FALSE, warning=FALSE}\n# just turn all NAs to 0 for now\n# test_input[is.na(test_input)] <- 0\n\nheat2 <- heatmap.3(test_input[,-1],\n                   hclustfun=myclust,\n                   distfun=mydist,\n                   na.rm = FALSE,\n                   scale=\"col\",\n                   dendrogram=\"row\",\n                   margins=c(5,8),\n                   Rowv=TRUE,\n                   Colv=TRUE,\n                   RowSideColors=rowcolz,\n                   #breaks = rampbreaks,\n                   #colsep = TRUE,\n                   #rowsep=TRUE,\n                   #sepcolor = 'gray30',\n                   #sepwidth = c(0.025,0.025),\n                   symbreaks=TRUE,\n                   #key=TRUE,\n                   symkey=TRUE,\n                   density.info=\"none\",\n                   trace=\"none\",\n                   #main='dh8',\n                   labCol=TRUE,\n                   labRow = TRUE,\n                   cexRow=0.33,\n                   col=rampcols,\n                   RowSideColorsSize=length(rowcolz[,1])#,\n                   #KeyValueName=\"fitness effect (s)\"\n                   )\n```\n\n# trying this shit with `ggplot2` since this heatmap functionality blows.\n```{r}\ninstall.packages(\"plotly\")\ninstall.packages(\"heatmaply\")\n```\n\n\n",
    "created" : 1524242027298.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4067585440",
    "id" : "3EBB0092",
    "lastKnownWriteTime" : 1524247000,
    "last_content_update" : 1524247000913,
    "path" : "~/Dropbox/PLT/analysis_PLT/PLT/FIG_PLANNING_V1.Rmd",
    "project_path" : "FIG_PLANNING_V1.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}