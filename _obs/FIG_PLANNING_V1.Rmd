---
title: "Figure thinking 10APR18"
output: html_notebook
---

This notebook details some figure design and sketch of analyses for displaying our main results.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
#install.packages("ggpubr")
library(ggpubr)
library(here)
library(dplyr)
library(MASS)

# set ggplot theme globally:
source(here("plt_themes.r"))
theme_set(theme_pth1())
```


## Part 1: data load and fitness parsing

Here we load up the data from Milo's latest fitness calculation routing. Data were downloaded as:

```
cd ~/Dropbox/PLT/analysis_PLT/PLT/data/
grdrive download 14Ny9xpvCbPPIWbJsHc5f1imLHIpspSiq --recursive
```

First, we load the data for each BFA, using the cutoff A/T run of 5:
```{r}
dBFA2 <- read.csv(here("data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/dBFA2_s_03_23_18_GC_cutoff_5.csv"))
hBFA1 <- read.csv(here("data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/hBFA1_s_03_23_18_GC_cutoff_5.csv"))
hBFA2 <- read.csv(here("data/Prelim_Fitness_Estimation_03_23_18/03_23_18_fitness_estimates/hBFA2_s_03_23_18_GC_cutoff_5.csv"))
```

For determining adaptedness: fit $t$ distribution for each environment and empirically calculate the quantile distribution and the correlation matrix among all BCs in the putative adaptive set.

Let's start with dBFA2. Fitness columns contain the string `iva_s`, so pull out indexes for each of these:
```{r}
# grab columns corresponding to inverse variance weighted (by rep) average fitness estimates
d2cols <- grep('iva_s', names(dBFA2), value = T)

# grab cols corresponding to the variance of these estimates
d2cols_err <- grep('_err', d2cols, value = T)

meta_cols <- names(dBFA2)[1:5] # define meta-data for BCs to capture
dBFA2_2     <- dBFA2[,(names(dBFA2) %in% c(meta_cols, d2cols)) & (!names(dBFA2) %in% d2cols_err)]
dBFA2_2_err <- dBFA2[,names(dBFA2) %in% c(meta_cols, d2cols_err)]
```

Use this to evaluate the neutrals across the environments. First let's plot them to see what's up. First let's produce long-form data so we can plot this stuff using `ggplot2`

```{r, message=FALSE, warning=FALSE}
# re-name columns:
d2cols2 <- gsub('.iva_s','', d2cols)
d3 <- reshape2::melt(dBFA2_2, value.name = 'iva.s', variable.name = 'bfa.env')

# bring over the error col as well
d3_err <- reshape2::melt(dBFA2_2_err, value.name = 'iva.s_err', variable.name = 'bfa.env')

# re-name bfa.env without 'iva_s'
d3$bfa.env <- gsub('\\.iva_s', '', d3$bfa.env)
d3_err$bfa.env <- gsub('\\.iva_s_err', '', d3_err$bfa.env)

# merge the two:
d4 <- merge(d3,d3_err,sort = F)

# change how iva.s and iva.s_err are reported, from per-cycle to per-generation:
d4$iva.s <- d4$iva.s / 8
d4$iva.s_err <- d4$iva.s_err / 8
```

The neutral set for dBFA2 corresponds to `d3$Subpool.Environment == 'Ancestor_YPD_2N'`. Let's capture these and plot their fitnesses across environments:
```{r}
d4n <- d4[d4$Subpool.Environment == 'Ancestor_YPD_2N',]

# calculate mean and sd (face value) of iva.s:
ggplot(d4n, aes(x = iva.s)) + geom_density() + facet_wrap( ~ bfa.env, scales = "free") + theme_bw()
```

Check if any entries are `NA`:
```{r}
sum(is.na(d4n$iva.s))
```

Most of these probably occur in `CLM`. Let's check:
```{r}
table(d4n[is.na(d4n$iva.s)==TRUE,'bfa.env'])[table(d4n[is.na(d4n$iva.s)==TRUE,'bfa.env'])>0]
```

Yep, OK. Basically, neutral clones go extinct or otherwise have terrible fitness estimates in these drug environments. To deal with this, we'll need to estimate a lower bound on fitness based on the read count data and assign these guys some fitness based on that. For now, however, we'll do something super cludgey, which is to assign these BCs a random fitness within $t$-distribution defined by the neutral set.

So, finally, let's plot these lineages, having initially removed these `NA` lineages:
```{r, fig.height=3, fig.width=5, message=FALSE, warning=FALSE}
d4n2 <- d4n[complete.cases(d4n),]
ggplot(d4n2, aes(x = bfa.env, y = iva.s, group = Full.BC, color = Which.Subpools)) + 
  geom_line() + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

We have a *clear* problem with fitness estimates for these lineages from the drug environment. The `R2` ancestral clones were spiked in a ten-fold lower frequency than the `R1` lineages, I believe, which may explain the larger than average swings in this sub-group.

Let's pretend this problem doesn't exist. We can calculate (by hand) the fitness estimates for the drug environments by calculating the posterior of the logit slopes between $t_8$ and $t_16$ (one transfer) and use the posteriors as a weighting factor to average the observed $s_{neut}^{env}$. We can probably do this rather quickly..

Let's spend a few minutes seeing whether we can do this, so that we have a roughly complete dataset to play with for generating figures. Loading up the BFA count data:

From the shell:
```
cd ~/Dropbox/PLT/analysis_PLT/PLT/data/
gdrive download 1omNcxLpV7KnHcWfHambx5R5_IqMvB8T2
```

Load data:
```{r}
dBFA2_counts <- read.csv(here("data/dBFA2_counts_with_env_info.csv"))
```

Add `total.count` for each column, corresponding to each time point, rep, and env:
```{r}
# define columns containing time-point BC counts
time.cols <- grep('Time', names(dBFA2_counts), value = T)

# generate long-form df:
d2_c2 <- reshape2::melt(dBFA2_counts,
                        value.name = 'count',
                        variable.name = 'bfa.env.rep.time',
                        measure.vars = time.cols)

# parse bfa.env.time col into bfa.env, rep, and time:
parse_names <- function(x){
  unlist(strsplit(paste0(x), '\\.'))
}

# split names; re-name cols; store as new columns to working counts df:
parsed_names_df <- data.frame(do.call(rbind, lapply(d2_c2$bfa.env.rep.time, parse_names)))[,-1]
names(parsed_names_df) <- c('bfa.env','bfa.rep','time')
d2_c3 <- cbind(d2_c2, parsed_names_df)
## this took way too long (~30 sec)... could make this faster with a series of matches. OH well.

# add back count_totals, for logit slope calculation:
# first calculate totals as colSums of time-point columns of original counts df
count_totals <- colSums(dBFA2_counts[,time.cols])

# turn this into a mergable df
dBFA2_totals_df <- data.frame(bfa.env.rep.time = names(count_totals), 'bfa.R' = as.vector(count_totals))

# merge with focal data
d2_c4 <- merge(d2_c3, dBFA2_totals_df, by = 'bfa.env.rep.time', sort = F)

# change the way time is encoded from character string to numeric vector:
d2_c4$time <- as.numeric(as.vector(gsub('Time','',d2_c4$time)))
```

Look at histogram of counts for the different sub-pools of the ancestral class:
```{r, fig.height=4, fig.width=10, message=FALSE, warning=FALSE}
d2_c4n <- d2_c4[d2_c4$Subpool.Environment == "Ancestor_YPD_2N",]

p1 <- ggplot(d2_c4n[(d2_c4n$bfa.env == 'FLC4') & (d2_c4n$time <= 16), ], aes(x = log(count,10), col = factor(time))) + 
  geom_density() + 
  facet_grid(Which.Subpools ~ bfa.rep) +
  theme_bw() + ggtitle("FLC") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

p2 <- ggplot(d2_c4n[(d2_c4n$bfa.env == 'CLM') & (d2_c4n$time <= 16), ], aes(x = log(count,10), col = factor(time))) + 
  geom_density() + 
  facet_grid(Which.Subpools ~ bfa.rep) +
  theme_bw() + ggtitle("CLM") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ggarrange(plotlist = list(p1,p2), labels = c('a','b'), align = 'hv')
```

There really is only one way to call fitness, which is if the starting frequency was sufficiently high to avoid hitting $r<10$ by $t_{16}$. We have very bad lower detection limits (scaled with frequency, yes).

The most heuristic and expedient way to correct for extinct barcodes is to add a single pseudo-count to all of the data, so that no barcode has `NA` fitness. This essentially enforces that all fitnesses estimated will be pinned at their lower bound, while noise in the non-zero count among the replicates will define the error in the measurements. For now let's only add the pseudocount to those BCs with $r_{t_{16}} = 0$ and then estimate the $\text{logit}$ slopes.
```{r}
# pass data from FLC4 and CLM to function:
d2_drug <- reshape2::dcast(d2_c4[d2_c4$bfa.env %in% c('FLC4','CLM'),],
                                  #bfa.env.rep.time + 
                                    Full.BC + 
                                    #Total.Counts + 
                                    Subpool.Environment + 
                                    Which.Subpools + 
                                    bfa.env + 
                                    bfa.rep ~ time, value.var = 'count')

# change names:
names(d2_drug)[match(c('8','16','24','32','40'), names(d2_drug))] <- c('r8','r16','r24','r32','r40')

d2_drug_R <- reshape2::dcast(d2_c4[d2_c4$bfa.env %in% c('FLC4','CLM'),],
                                  #bfa.env.rep.time + 
                                    Full.BC + 
                                    #Total.Counts + 
                                    Subpool.Environment + 
                                    Which.Subpools + 
                                    bfa.env + 
                                    bfa.rep ~ time, value.var = 'bfa.R')

# change names:
names(d2_drug_R)[match(c('8','16','24','32','40'), names(d2_drug_R))] <- c('R8','R16','R24','R32','R40')

# merge two dfs:
# need to ensure that reshape has conducted itself identically between dfs so I can simply cbind them together..
# should be all TRUE:
table(paste0(d2_drug$Full.BC,d2_drug$bfa.env,d2_drug$bfa.rep) == paste0(d2_drug_R$Full.BC,d2_drug_R$bfa.env,d2_drug_R$bfa.rep))

# OK Yep. Just cbind() the dfs together:
d2_drug <- cbind(d2_drug, d2_drug_R[,c('R8','R16','R24','R32','R40')])
```

How many barcodes hit $r_{16} = 0$? 
```{r}
length(d2_drug[,'r16'][!d2_drug[,'r16'] > 0]) / length(d2_drug[,1])
```

That's a huge proportion. Yikes. OK oh well! Let's go ahead and add the pseudo-count and take the logit slopes, calculated as
$$s = 
\dfrac{1}{T} 
\times \Bigg[ 
\text{ln} \Big( \dfrac{f_t}{1-f_t} \Big) - \text{ln} \Big( \dfrac{f_0}{1-f_0} \Big) \Bigg]$$

where $T$ is the elapsed number of generations.
```{r}
# add pseudo-count
# we will only do this for t16, since those with too few counts at t8 have already been excluded
d2_drug[,'r16'][!d2_drug[,'r16'] > 0] <- 1

# def logit slope function
calc_logit_slopes <- function(x, t1='r8', t2='r16', R1 = 'R8', R2 = 'R16', TT = 8){
  f0 <- x[,t1] / x[,R1]
  f1 <- x[,t2] / x[,R2]
  s <- (1/TT) * ( log( f1 / (1 - f1) ) - log( f0 / (1 - f0) ) )
  return(s)
}

# run script to calculate for all rows:
d2_drug$s_est <- calc_logit_slopes(d2_drug)
```

Let's give this a plot, specifically for the neutral class:
```{r, fig.height=3.5, fig.width=5, message=FALSE, warning=FALSE}
dp1 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'Ancestor_YPD_2N',], aes(x = s_est, col = bfa.rep)) +
  geom_density() + 
  facet_grid(bfa.env ~ Subpool.Environment) + 
  theme_bw()

dp1
```

We can see that the zero-counts which received the +1 pseudocount typically exist in a lower mode of fitness estimates. I'm not sure of any reasonable way to up-shrink these estimates. I imagined that the entire distribution would be unimodal, but I was wrong. This probably has to do with the very large noise properties in these assays. Additionally, the three replicates are different from one another in quite dramatic ways, especially for `FLC4`: I should basiclaly toss out `R3` since the read counts are so low.

This is a problem, so much so that we may have to re-do this BFA so we get more reasonable coverage and less extreme swings in frequency. We can discuss this on the phone on Wednesday.

Before we move on, let's take a look at the other source environments, specifically `FLC4` (a) and `CLM` (b) home environments:
```{r, fig.height=2.5, fig.width=7, message=FALSE, warning=FALSE}
dp2 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'FLC4_2N',], aes(x = s_est, col = bfa.rep)) + 
  geom_density() + 
  facet_wrap( ~ bfa.env) +
  ggtitle('FLC4')

dp3 <- ggplot(d2_drug[d2_drug$Subpool.Environment == 'CLM_2N',], aes(x = s_est, col = bfa.rep)) + 
  geom_density() + 
  facet_wrap( ~ bfa.env) + 
  ggtitle('CLM')

ggarrange(plotlist = list(dp2,dp3), labels = c('a','b'), align = 'hv', common.legend = T)
```

These have not been corrected for neutral fitness, so you can't compare location of distributions to get information about distributions of fitness effects.

What's next?

1. Organize dBFA2 data for all relevant environments, subtracting off the crazy drug stuff
2. Do the same with hBFA1 data
3. determine adapted for all environments
 
Goal is to have some kind of dendrogram figure to compile with some other JDFE-style figures to show folks tomorrow.

## *prima facie* analysis of fitness patterns

Let's pretend we don't care about the incredibly wonky fitness patterns produced in the drug environemnts. We'll simply take the complete sub-set of the data in order to generate mock-up plots of what we might want our analyses to look like. First up depends on us determining adapted sub-set and clustering them based on their fitness traces.

Let's first start by calling adapted. For `dBFA2` we will ignore the drug environments and then simply take the $s$ estimates for these environments as given for all the lineages defined as adapted on the basis of all the other environments. To do this, for all lineages for all bfa.env, we calculate the empirical quantile of estimated fitness in the distribution of neutral class estimates. We do the same for the neutrals, which constitutes our testing distribution. We will then estimate some form of cutoff for which false discovery rate is empirically adjusted to ~5% across the dataset. We could go lower, too.

Use `data.frame` `d4` for all of this downstream stuff.
```{r}
# for all lineages for all bfa.env, calculate empirical quantile of estimated fitness in the distribution of neutral class estimates.
# fastest way will be to derive testing distribution for each bfa.env combination.. excluding CLM and FLC4

# define bfa.env set I want to look through
envs <- unique(d4$bfa.env)
envs <- envs[!envs %in% c('FLC4','CLM')]

# pass vector of barcodes as neutral set
neuts <- unique(d4[d4$Subpool.Environment == 'Ancestor_YPD_2N','Full.BC'])

# define function to do loop, so I can recylce it for each BFA
calc_neut_t <- function(x, ENVS, NEUTS){
  # define empty results df
  res <- data.frame()
  
  # loop through environments
  for (e in 1:length(ENVS)){
    # extract neutral lineages
    neut_BCs <- x[(x$Full.BC %in% NEUTS) & (x$bfa.env == ENVS[e]),]
    
    # remove any NA or any Inf:
    neut_BCs <- neut_BCs[!neut_BCs$iva.s == Inf,]
    
    # calculate weighted average and weighted sigma for neutral fitness:
    # fist, define weights.
    # do so proportional to 1/sigma, standardized by sum
    w <- 1/neut_BCs$iva.s_err
    
    # calculate sum of weights:
    sw <- sum(1/neut_BCs$iva.s_err)
    
    # calculate (ML) weighted sum, standardized to sum of weights:
    w_mu <- sum(neut_BCs$iva.s * w) / sw 
    
    # calculate (ML) weighted standard deviation of this sample
    w_sd <- sqrt(sum((w/sw) * (neut_BCs$iva.s - w_mu)^2))
    
    # return the results for the relevant bfa.env:
    res <- rbind(res, data.frame(bfa.env = ENVS[e], w_mu = w_mu, w_sd = w_sd))
  }
  
  # merge back to main df, matching by bfa.env
  x2 <- merge(x, res, by = 'bfa.env', sort = F, all.x = T)
  
  # calculate test statistic for all barcodes:
  x2$t_star  <- (x2$iva.s - x2$w_mu) / (x2$iva.s_err + x2$w_sd)
  
  # output the modified df
  return(x2)
}

d5 <- calc_neut_t(d4, ENVS = envs, NEUTS = neuts)
```

Let's look into excluding any barcodes with super wonky errors (compared to their estimates):
```{r, fig.height=2, fig.width=6, message=FALSE, warning=FALSE}
d4$s_s_err <- abs(d4$iva.s_err)

d4b <- dplyr::group_by(d4[!d4$bfa.env %in% c('FLC4','CLM'),], Full.BC) %>% summarise(mean_s_s_err = mean(s_s_err, na.rm = T), err_max = max(s_s_err, na.rm = T), err_min = min(s_s_err, na.rm = T)) %>% arrange(mean_s_s_err)

d4b$Full.BC <- factor(d4b$Full.BC, levels = unique(paste0(d4b$Full.BC)))
d4b$neutral <- 0
d4b$neutral[d4b$Full.BC %in% neuts] <- 1

ggplot(d4b, aes(x = Full.BC, y = mean_s_s_err, ymax = err_max, ymin = err_min)) + geom_linerange(col = "gray80") +
  geom_point(col = "gray40", size = 0.75) + facet_wrap(~ neutral) +
  theme(axis.text.x = element_blank(),
        axis.line.x = element_blank(),
        axis.ticks.x = element_blank()) + xlab("")
```

There aren't any neutrals with wonky errors (perhaps a couple; these will throw off the critical value estimation, below..). Wouldn't be a bad idea to cut out this top 1% that looks terrible.

Anyhow.

Let's make some plots of the t-scores, especially of the neutral lineages themselves. We can iterate and discard any wonky looking guys, else include them as part of the false discovery set.
```{r, fig.height=3.5, fig.width=5, message=FALSE, warning=FALSE}
ggplot(d5[d5$Full.BC %in% paste0(neuts),], aes(x = t_star)) + geom_density() + facet_wrap(~ bfa.env, scales = 'free')
```

These look like fairly reasonable testing distributions. Let's plot the distribution of the sum of the abs(t-scores):
```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
# need to exclude FLC4 and CLM for these calculations:
d5b <- d5[!d5$bfa.env %in% c('FLC4','CLM'),]
tsums <- dplyr::group_by(d5b, Full.BC) %>% summarise(t_chi = sum(t_star^2), n_envs = length(t_star)) %>% mutate(t_chi_avg = t_chi / n_envs)

t_sum_dist_p <- ggplot(tsums[tsums$Full.BC %in% paste0(neuts),], aes(x = t_chi)) + geom_density() + theme_bw() + ggtitle("neutral t_Chisq dist") +
  scale_x_continuous(limits = c(0,20))
t_sum_dist_p
```

Let's fit (using ML) the $df$ parameter of the $\chi^2$ distribution to these data and inspect the fit:
```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
# fit Chi-squared distribution to this, truncating at the upper part (i.e. >10):
chis <- as.data.frame((tsums[tsums$Full.BC %in% paste0(neuts),'t_chi']))
emp_df <- fitdistr(chis$t_chi[chis$t_chi != Inf], "chi-squared", start = list(df = 1))

# make vector of densities for Chisq with df = emp_df$estimate
chis_dist <- data.frame(dchi = dchisq(seq(0,20,0.01), df = emp_df$estimate), est = seq(0,20,0.01))

# plot as overlap on previous graph:
t_sum_dist_p <- t_sum_dist_p + geom_line(data = chis_dist, aes(x = est, y = dchi), col = "darkorange", lty = "dashed") 
t_sum_dist_p
```

So, the fit isn't perfect, but it looks like we have something approximating a $\chi^2$ distribution for our neutral barcodes. We might as well use the empirical quantile just so we don't accidentally accept barcodes as adapted if their `t_chi` is close to zero (the point at which the densitites in the plot diverge maximally). So, we find the quantile which gives a FDR of 5%:

```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
fdr <- quantile(chis$t_chi, probs = c(0.95))
#fdr <- cbind(sort(chis$t_chi),seq(1:length(chis$t_chi))/length(chis$t_chi))

# find the 95% quantile of the Chisq with optimized df:
chi95 <- qchisq(0.95, df = emp_df$estimate)

t_sum_dist_p <- t_sum_dist_p + geom_vline(xintercept = fdr, col = "midnightblue") + geom_vline(xintercept = chi95, col = "magenta")
t_sum_dist_p
```
Where the magenta cutoff is the theoretical critical value and the darkblue line is the empirical one.

If we removed the bizarre-looking outliers we would probably converge back to the $\chi^2$ critical value corresponding to $\alpha = 0.05$. Our empirical cutoff is substantially higher. For now, however, we will use out empirical cutoff in order to call lineages as adaptive or not:
```{r, fig.height=2, fig.width=10, message=FALSE, warning=FALSE}
tsums$adapted <- 0
tsums$adapted[tsums$t_chi >= fdr] <- 1

tsums$neutral <- 0
tsums$neutral[tsums$Full.BC %in% paste0(neuts)] <- 1

# bring back in source.env information:
tsum2 <- merge(tsums, unique(d5[,c('Full.BC','Diverse.BC','Subpool.Environment')]), by = 'Full.BC', sort = F)

# exclude spurious environment calls:
tsum2 <- tsum2[!tsum2$Subpool.Environment %in% c('none','not_read'),]
# write as csv for now:
write.csv(tsum2, file = here("data/dBFA2_adapted_calls_20-APR-2018.csv"), quote = F, row.names = F)

# plot these sums by bfa.env and source env
ggplot(tsum2, aes(x = factor(adapted), y = log(t_chi), col = factor(adapted))) + 
  geom_jitter(width = 0.1, alpha = 0.1, aes(col = factor(adapted))) +
  geom_boxplot(alpha = 0.5) + 
  facet_wrap(~ Subpool.Environment, scales = 'free', ncol = 12) +
  theme_bw() + scale_color_manual(values = c('gray60','magenta2')) + 
  theme(legend.position = 'none', strip.background = element_blank()) +
  xlab('adapted') + ylab('log(chisq)')
```

Now try again with density plots:
```{r}
ttt <- data.frame(table(tsum2$adapted,tsum2$Subpool.Environment)) %>% dplyr::arrange(desc(Var1),desc(Freq))

# scrub levels that don't make sense; re-order:
ttt <- ttt[!ttt$Var2 %in% c('none','not_read'),]
ttt$Var2 <- factor(ttt$Var2, levels = paste0(unique(ttt$Var2)))

# supply text annotations:
ttt2 <- reshape2::dcast(ttt, Var2 ~ Var1, value.var = 'Freq')
names(ttt2) <- c('source','non_adapted','adapted')
ttt2 <- dplyr::mutate(ttt2, perc = adapted / (adapted + non_adapted))

# produce plot
clone_counts_2N <- ggplot() + 
  geom_bar(data = ttt, aes(x = Var2, y = Freq, fill = Var1), stat = 'identity', position = position_dodge(0.5), alpha = 0.5) +
  theme_pth1() + scale_fill_manual(values = c('gray40','dodgerblue'), name = 'adapted') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  ylab("number of unique BCs with fitness data") + 
  xlab("source environment") +
  annotate(geom = "text", label = paste0(round(ttt2$perc,2)), x = ttt2$source, y = -20, size = 2)

# save:

ggsave(filename = here("plt_figs/2N_adapted_clones_breakdown.pdf"), width = 5, height = 3.5)
```

Now plot the density plots of the t_sum by source
```{r}
ggplot(tsum2, aes(x = log(t_chi_avg), col = factor(adapted))) + geom_density() + facet_wrap(~ Subpool.Environment)
```

This seems like a perfectly reasonable way to get things moving. Clearly there is some threshold that is continuously met at the cutoff, so we may decide to implement some kind of Bayesian classifier that optimizes both positive and negative predictive value of the classifier. We'll leave that for a future iteration of the analysis.

Let's count up the adapted clones from each source environment:
```{r}
aBCs <- tsums$Full.BC[tsums$adapted==1]
d5$adapted <- 0
d5$adapted[d5$Full.BC %in% aBCs] <- 1
d5$neutral <- 0
d5$neutral[d5$Full.BC %in% paste0(neuts)] <- 1

# remove spurious BCs:
d5 <- droplevels(d5[!d5$Subpool.Environment %in% c('none','not_read'),])
t1 <- table(d5$adapted, d5$Subpool.Environment)
t1
```

Plot this:
```{r, fig.height=3, fig.width=4, message=FALSE, warning=FALSE}
t3 <- reshape2::melt(t(data.frame(apply(t(t1),1,function(x) x/sum(x))))) %>% dplyr::arrange(desc(Var2), desc(value))
names(t3) <- c('source','adapted','prop.adapt')
t3$source <- factor(t3$source, levels = paste0(t3$source))

# re-sort t3 based on prop adaptive:
prop_adapt_p <- ggplot(t3, aes(x = source, y = prop.adapt, fill = factor(adapted))) + 
  geom_bar(stat = 'identity') + scale_fill_manual(values = c('gray90','dodgerblue')) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_hline(yintercept = 0.05, lty = 'dashed', col = 'darkorange')
prop_adapt_p
```
Dashed line is 5%, and the neutral class is right where it should be (with 5% called as adapted).

Let's plot the spagetti plot for the relevant source environments. We'll exclude 48hr, and we'll also exclude the 37C at Stanford `bfa.env`:

```{r}
d6 <- droplevels(d5[d5$bfa.env != 'X37C_Stan',])
d6 <- droplevels(d6[d6$Subpool.Environment != '48Hr_2N',])
ggplot(d6, aes(x = bfa.env, y = iva.s, group = Full.BC, col = Subpool.Environment)) + 
  geom_line(alpha = 0.4) + 
  #theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~ Subpool.Environment)
```

**Next steps include:**
  1. building dendrogram of these data
  2. bringing in hBFA1 data. Should be reasonably fast.
  3. Plotting with source env. and ploidy as factors. Sub-sampling the clusters for clarity.
  4. Calculating summary pleiotropy statistics for the clusters.
  
## Producing heatmaps

*What sort of meta-data do I want to plot?*

1. Source environment
2. Ploidy

For now I can quickly bring over some haploid data just for the figure mock-up. Let's maybe quickly try to bring in hBFA1 data, actually:

## hBFA1 data parsing

```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
### hBFA1 data parsing routine:
# grab columns corresponding to inverse variance weighted (by rep) average fitness estimates
h1cols <- grep('iva_s', names(hBFA1), value = T)

# grab cols corresponding to the variance of these estimates
h1cols_err <- grep('_err', h1cols, value = T)

meta_cols <- names(hBFA1)[1:5] # define meta-data for BCs to capture
hBFA1_2     <- hBFA1[,(names(hBFA1) %in% c(meta_cols, h1cols)) & (!names(hBFA1) %in% h1cols_err)]
hBFA1_2_err <- hBFA1[,names(hBFA1) %in% c(meta_cols, h1cols_err)]

# re-name columns:
h1cols2 <- gsub('.iva_s','', h1cols)
h3 <- reshape2::melt(hBFA1_2, value.name = 'iva.s', variable.name = 'bfa.env')

# bring over the error col as well
h3_err <- reshape2::melt(hBFA1_2_err, value.name = 'iva.s_err', variable.name = 'bfa.env')

# re-name bfa.env without 'iva_s'
h3$bfa.env <- gsub('\\.iva_s', '', h3$bfa.env)
h3_err$bfa.env <- gsub('\\.iva_s_err', '', h3_err$bfa.env)

# merge the two:
h4 <- merge(h3,h3_err,sort = F)

# change how iva.s and iva.s_err are reported, from per-cycle to per-generation:
h4$iva.s <- h4$iva.s / 8
h4$iva.s_err <- h4$iva.s_err / 8

# define bfa.env set I want to look through
henvs <- unique(h4$bfa.env)
henvs <- henvs[!henvs == 'X48Hr'] # remove 48Hr environment

# pass vector of barcodes as neutral set
hneuts <- unique(h4[h4$Subpool.Environment == 'YPD_alpha','Full.BC'])

# calc t-score relative to neutrals
h5 <- calc_neut_t(h4, ENVS = henvs, NEUTS = hneuts)

# plot sum of squared t-scores:
#h5b <- h5[!h5$bfa.env %in% c('FLC4','CLM'),]

htsums <- dplyr::group_by(h5, Full.BC) %>% summarise(t_chi = sum(t_star^2, na.rm = T), n_envs = length(t_star)) %>% mutate(t_chi_avg = t_chi / n_envs)

ht_sum_dist_p <- ggplot(htsums[htsums$Full.BC %in% paste0(hneuts),], aes(x = t_chi)) + geom_density() + theme_bw() + ggtitle("neutral t_Chisq dist") +
  scale_x_continuous(limits = c(0,5))
ht_sum_dist_p
```

```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
chis <- as.data.frame((htsums[htsums$Full.BC %in% paste0(hneuts),'t_chi']))
emp_df <- fitdistr(chis$t_chi[chis$t_chi != Inf], "chi-squared", start = list(df = 1))

# make vector of densities for Chisq with df = emp_df$estimate
chis_dist <- data.frame(dchi = dchisq(seq(0,20,0.01), df = emp_df$estimate), est = seq(0,20,0.01))

# plot as overlap on previous graph:
ht_sum_dist_p <- ht_sum_dist_p + geom_line(data = chis_dist, aes(x = est, y = dchi), col = "darkorange", lty = "dashed") 
ht_sum_dist_p
```

```{r, fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
# this fit is way worse that for dBFA2. Not sure what's up. Maybe fit a non-centrality parameter?

fdr <- quantile(chis$t_chi, probs = c(0.95))
#fdr <- cbind(sort(chis$t_chi),seq(1:length(chis$t_chi))/length(chis$t_chi))

# find the 95% quantile of the Chisq with optimized df:
chi95 <- qchisq(0.95, df = emp_df$estimate)

ht_sum_dist_p <- ht_sum_dist_p + geom_vline(xintercept = fdr, col = "midnightblue") + geom_vline(xintercept = chi95, col = "magenta")
ht_sum_dist_p
```

The dark blue line (empirical cutoff) is less conservative than the $\chi^2$ 95% critical value, but since we see that it fits so poorly, we'll stick to the empirical cutoff here, as we did for dBFA2 above.

```{r}
htsums$adapted <- 0
htsums$adapted[htsums$t_chi >= fdr] <- 1

htsums$neutral <- 0
htsums$neutral[htsums$Full.BC %in% paste0(hneuts)] <- 1

# bring back in source.env information:
htsum2 <- merge(htsums, unique(h5[,c('Full.BC','Diverse.BC','Subpool.Environment')]), by = 'Full.BC', sort = F)

# write this shit to file:
write.csv(htsum2, file = here("data/hBFA1_adapted_calls_20-APR-2018.csv"), quote = F, row.names = F)

# exclude spurious environment calls:
htsum2 <- htsum2[!htsum2$Subpool.Environment %in% c('none','not_read'),]

haBCs <- htsums$Full.BC[htsums$adapted==1]
h5$adapted <- 0
h5$adapted[h5$Full.BC %in% haBCs] <- 1
h5$neutral <- 0
h5$neutral[h5$Full.BC %in% paste0(hneuts)] <- 1

# remove spurious BCs:
h5 <- droplevels(h5[!h5$Subpool.Environment %in% c('none','not_read'),])
ht1 <- table(h5$adapted, h5$Subpool.Environment)
ht1
```

```{r, fig.height=3, fig.width=4, message=FALSE, warning=FALSE}
ht3 <- reshape2::melt(t(data.frame(apply(t(ht1),1,function(x) x/sum(x))))) %>% dplyr::arrange(desc(Var2), desc(value))
names(ht3) <- c('source','adapted','prop.adapt')
ht3$source <- factor(ht3$source, levels = paste0(ht3$source))

# re-sort t3 based on prop adaptive:
hprop_adapt_p <- ggplot(ht3, aes(x = source, y = prop.adapt, fill = factor(adapted))) + 
  geom_bar(stat = 'identity') + scale_fill_manual(values = c('gray90','dodgerblue')) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_hline(yintercept = 0.05, lty = 'dashed', col = 'darkorange')
hprop_adapt_p
```

Spit out figure for dBFA2 and hBFA1 side by side:

```{r, fig.height=3, fig.width=5, message=FALSE, warning=FALSE}
ggarrange(plotlist = list(prop_adapt_p,hprop_adapt_p), labels = c('a','b'), align = 'hv', common.legend = T,
          widths = c(1,0.9)) %>% ggsave(filename = here("plt_figs/prop_adapted_hap_dip.pdf"),width=5, height = 3.5)

ggarrange(plotlist = list(prop_adapt_p,hprop_adapt_p), labels = c('a','b'), align = 'hv', common.legend = T,
          widths = c(1,0.825))
```

OK not bad! These haven't been corrected for diploids though.

### Exporting working dBFA2 fitness file

```{r}
# split dh5 into two: one with meta-data, the other with fitnesses, then re-join:
d5$sources <- gsub('_2N','',d5$Subpool.Environment)

# now define ploidy state
d5$ploidy <- '2N'

# flag if auto-diploid:
d5$subpool <- 'R2'
d5$subpool[grep('-R1',d5$Which.Subpools)] <- 'R1'

d5_meta <- unique(d5[,names(d5) %in% c('Full.BC','adapted','neutral','source','ploidy','subpool')])
d5_2 <- d5[,names(d5) %in% c('Full.BC','sources','bfa.env','iva.s')]

# currently this call isn't working:
d5_long <- reshape2::dcast(d5_2, Full.BC + sources ~ bfa.env, value.var = 'iva.s', fun.aggregate = mean)

# add back meta-data:
d5_long2 <- merge(d5_long,d5_meta, by = 'Full.BC', sort = F)

write.csv(d5_long2, file = here('data/dBFA2_fitnesses_with_adapt_20APR2018.csv'), quote = F)
```


## Putting together a joint dendrogram from both assays

We start by combining the adapted BCs from dBFA2 and hBFA1 into a common df. We then cluster and plot as a dendrogram, with ploidy as a factor, as well as source environment. We may choose to down-sample barcodes belonging to the same clusters, which we will define using `cuttree` once we have the clust object to work tith.
```{r}
# combining dfs:
# using d5 and h5 as input dfs:
dh5 <- rbind(d5[d5$adapted==1,],h5[h5$adapted==1,])

# Let's add back a handul of neutral guys from both dBFA2 and hBFA1
d5n <- d5[d5$neutral==1,]
h5n <- h5[h5$neutral==1,]

# sample neutral BCs from each BFA:
ns <- 100
d.samp <- sample(unique(d5n$Full.BC),ns, replace = F)
h.samp <- sample(unique(h5n$Full.BC),ns, replace = F)

d5n2 <- d5n[d5n$Full.BC %in% d.samp,]
h5n2 <- h5n[h5n$Full.BC %in% h.samp,]

# add back to dh5"
dh5 <- rbind(dh5,rbind(d5n2,h5n2))

# split Subpool.Environment into source and ploidy:
sources <- gsub('_2N','',dh5$Subpool.Environment)
sources <- gsub('_alpha','',sources)
# add it back in
dh5$source <- sources

# now define ploidy state
dh5$ploidy <- '2N'
dh5$ploidy[grep('alpha',dh5$Subpool.Environment)] <-'1N'

# flag if auto-diploid:
dh5$subpool <- 'R2'
dh5$subpool[grep('-R1',dh5$Which.Subpools)] <- 'R1'

dh5$autodip <- 0
dh5$autodip[grep('autodiploids',dh5$Which.Subpools)] <- 1

# write this to file:
write.csv(dh5, file = here('data/dBFA2_hBFA1_with_adapt_11APR2018.csv'), quote = F)
```

Now we're ready to plot a dendrogram. We need to convert our joint df into wide-format with `dcast`:
```{r}
# remove 48Hr from source and bfa.env
dh5 <- dh5[dh5$source != '48Hr',]
dh5 <- dh5[dh5$bfa.env != 'X48Hr',]
dh5 <- dh5[dh5$bfa.env != 'X37C_Stan',]

dh5$source <- factor(dh5$source)
dh5$bfa.env <- factor(dh5$bfa.env)

# split dh5 into two: one with meta-data, the other with fitnesses, then re-join:
dh5_meta <- unique(dh5[,names(dh5) %in% c('Full.BC','adapted','neutral','source','ploidy','subpool','autodip')])
dh6 <- dh5[,names(dh5) %in% c('Full.BC','bfa.env','iva.s')]

# currently this call isn't working:
dh7 <- reshape2::dcast(dh6, Full.BC ~ bfa.env, value.var = 'iva.s', fun.aggregate = mean)

# add back meta-data:
dh8 <- merge(dh7,dh5_meta, by = 'Full.BC', sort = F)
row.names(dh8) <- dh8$Full.BC

# bfa.env names:
bfa.envs <- names(dh8)[2:11]

# write dh8 to file for convenience:
write.csv(dh8, here("data/dBFA2_hBFA1_fitnesses_allBCs.csv"), row.names = F, quote = F)
```


<!-- ```{r} -->
<!-- # ncol <- 20 -->
<!-- # cols <- RColorBrewer:::brewer.pal(11,"PuOr") -->
<!-- # rampcols <- colorRampPalette(colors = cols, space="Lab")(ncol) -->
<!-- # rampbreaks <- seq(0, 100, length.out = ncol+1) -->

<!-- ## more stuff leading up to dendro: -->
<!-- # define custom ramp: -->
<!-- nn <- 16 -->
<!-- rc1 = colorRampPalette(colors = c("#f1a340", "white"), space="Lab")(nn) -->
<!-- ## Make vector of colors for values above threshold -->
<!-- rc2 = colorRampPalette(colors = c("white", "#998ec3"), space="Lab")(nn) -->
<!-- rampcols = c(rc1, rc2) -->
<!-- ## In your example, this line sets the color for values between 49 and 51. -->
<!-- rampcols[c(nn, nn+1)] = rgb(t(col2rgb("white")), maxColorValue=256) -->

<!-- # Min = min(Jall_new3) -->
<!-- # Max = max(Jall_new3) -->
<!-- Min = -0.3 -->
<!-- Max = 0.3 -->
<!-- Thresh = 0 -->

<!-- rb1 = seq(Min, Thresh, length.out=nn+1) -->
<!-- rb2 = seq(Thresh, Max, length.out=nn+1)[-1] -->
<!-- rampbreaks = c(rb1, rb2) -->
<!-- ``` -->

Install `heatmap.3` function:
```{r}
install.packages("devtools")
library("devtools")
source_url("https://raw.githubusercontent.com/obigriffith/biostar-tutorials/master/Heatmaps/heatmap.3.R")
```

<!-- ```{r} -->
<!-- random_rows <- sample(seq(1,length(dh8[,1]),1),size = 250, replace=F) -->
<!-- test_input <- dh8[random_rows,bfa.envs] -->
<!-- test_input <- test_input[complete.cases(test_input),] -->

<!-- test_meta <- dh5_meta[dh5_meta$Full.BC %in% test_input$Full.BC,] -->
<!-- #row.names(test_meta) <- test_meta$Full.BC -->
<!-- #test_meta <- test_meta[,-1] -->

<!-- test_meta$source <- factor(test_meta$source) -->
<!-- test_meta$ploidy <- factor(test_meta$ploidy) -->

<!-- # need to color all of the meta-data: -->
<!-- #library(viridis) -->
<!-- scolors <- -->
<!--    with(test_meta, -->
<!--         data.frame(source = levels(source), -->
<!--                    scolor = I(viridis(nlevels(source))))) -->

<!-- pcolors <- -->
<!--    with(test_meta, -->
<!--         data.frame(ploidy = levels(ploidy), -->
<!--                    pcolor = I(magma(nlevels(ploidy))))) -->


<!-- test_meta <- merge(test_meta, scolors, by = 'source', sort = F) -->
<!-- test_meta <- merge(test_meta, pcolors, by = 'ploidy', sort = F) -->
<!-- #test_meta$source <- relevel(test_meta$source, levels = sample(viridis(12), length(unique(test_meta$source)))) -->
<!-- row.names(test_meta) <- test_meta$Full.BC -->
<!-- test_meta <- test_meta[,-1] -->

<!-- rowcolz <- rbind(paste0(test_meta$scolor),paste0(test_meta$pcolor)) -->
<!-- ``` -->

Heatmap-specific functions:
```{r}
Inf_to_NA <- function(x){
  # find rows with Inf:
  inf.rows <- apply(x[,-1],1,function(x) sum(is.infinite(x)))
  
  # find submatrix with Infs:
  sm <- x[inf.rows>0 , ]
  
  # traverse rows
  for (r in 1:length(sm[,1])){
    sm[r,-1][is.infinite(as.numeric(as.vector(sm[r,-1])))] <- NA
  }
  
  # substitute Inf-->NA rows back into input df
  x[inf.rows>0,] <- sm
  
  # output modified input df
  return(x)
}

# define custom dist and clust functions for use with heatmap.3
#mydist=function(c) {daisy(Inf_to_NA(c))}
mydist=function(c) {dist(c, method = 'euclidean')}
myclust=function(c) {hclust(c, method="average")}
```


Let's make a heatmap, with all of the lineages:
```{r}
#random_rows <- sample(seq(1,length(dh8[,1]),1),size = 250, replace=F)
bfa.envs <- bfa.envs[!bfa.envs %in% c('X02M_NaCl','X37C_Stan')]
test_input <- dh8[,bfa.envs]

# turn all Inf values to NA
test_input <- Inf_to_NA(test_input)

# remove all BCs with >1 NA
NAs <- apply(test_input[,-1], 1, function(x) sum(is.na(x)))
test_input <- test_input[!NAs>1,]

# FOR NOW:
# turn all NAs into zeros so I can actually get through this heatmap plotting
test_input[is.na(test_input)] <- 0
```

Let's do some scaling. For visual display, I'd like to plot all of the data on the same relative scale, standardized to between min and max, and mean-centered.
I will also include meta-data for the bfa.envs, where I plot the mean and variance (i.e., standard deviation) of fitness effects (untransformed) as a single dimension heatmap.

```{r}
# re-scaling:
# x <- test_input[,bfa.envs[4]]
# mx <- mean(x)
# minx <- min(x)
# maxx <- max(x)
# sdx <- sd(x)

### obsolete:
# re_scale <- function(x){
  #   rank(x)/length(x)
  #   (x - mean(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T))  
  # }

# function to re-scale by quantiles for positive and negative range separately:
# (signed quantile rescaling)
sqr_by_env <- function(x){
  # define function to calculate signed quantile rescaling
  sqr <- function(x){
    x2 <- x>0 #take positive
    x3 <- x<0 #take negatives
    # re-scale positives
    x[x2] <- rank(x[x2]) / length(x[x2])
    x[x3] <- (-1) * rank(abs(x[x3])) / length(x[x3])
    return(x)
  }
  
  # apply function over all input columns separately
  x2 <- do.call(cbind, lapply(x, sqr))
  
  # return transformed 
  return(x2)
}

# try out:
test_in2 <- sqr_by_env(test_input[, bfa.envs[-1]])
```


Generate plots to correspond the rank to the actual fitness values:
```{r, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
test_in3 <- cbind(reshape2::melt(test_in2, value.name = 'quantile'),
                  reshape2::melt(test_input[, bfa.envs[-1]], value.name = 'fitness'))

ggplot(test_in3, aes(x = fitness, y = quantile)) + geom_line() + facet_wrap(~ Var2) + 
  geom_vline(xintercept = 0, col = 'gray60', lty = 'dotted') +
  geom_hline(yintercept = 0, col = 'gray60', lty = 'dotted')
```

This plot shows the relationship between signed quantile and actual fitness. We will be plotting quantiles in the heatmap, each of which corresponds to a different extent of variation both above and below ancestral fitness.

```{r}
# capture resulting BCs and grab their meta-data:
# test_meta <- dh8_meta[dh5_meta$Full.BC %in% test_input$Full.BC,]
meta_cols <- c('source','ploidy','subpool','autodip','neutral')
test_meta <- droplevels(dh8[dh8$Full.BC %in% test_input$Full.BC, meta_cols])

#### define colors ####
test_meta$source <- factor(test_meta$source)
test_meta$subpool <- factor(test_meta$subpool)
test_meta$ploidy <- factor(test_meta$ploidy)
test_meta$autodip <- factor(test_meta$autodip)
test_meta$neutral <- factor(test_meta$neutral)

# define source colors (12-level):
scolors <-
   with(test_meta,
        data.frame(source = levels(source),
                   scolor = RColorBrewer::brewer.pal(length(levels(source)), "Paired")))

# define ploidy colors (two-level):
pcolors <-
   with(test_meta,
        data.frame(ploidy = levels(ploidy),
                   pcolor = c('#deebf7','#3182bd')))

# define neutral colors (two-level):
ncolors <- with(test_meta,
        data.frame(neutral = levels(neutral),
                   ncolor = c('#f0f0f0','#636363')))

# define autodiploid colors (two-level):
acolors <- with(test_meta,
        data.frame(autodip = levels(autodip),
                   acolor = c('#efedf5','#de2d26')))

# add back to meta data:
test_meta <- merge(test_meta, scolors, by = 'source', sort = F)
test_meta <- merge(test_meta, pcolors, by = 'ploidy', sort = F)
test_meta <- merge(test_meta, acolors, by = 'autodip', sort = F)
test_meta <- merge(test_meta, ncolors, by = 'neutral', sort = F)

#test_meta$source <- relevel(test_meta$source, levels = sample(viridis(12), length(unique(test_meta$source))))
row.names(test_meta) <- test_meta$Full.BC
test_meta <- test_meta[,-1]

rowcolz <- rbind(paste0(test_meta$scolor),
                 #paste0(rep("white", length(test_meta$scolor))), # add whitespace col between factor
                 paste0(test_meta$pcolor),
                 #paste0(rep("white", length(test_meta$scolor))), # add whitespace col between factor
                 paste0(test_meta$acolor),
                 #paste0(rep("white", length(test_meta$scolor))), # add whitespace col between factor
                 paste0(test_meta$ncolor))
```

Below are parameters for heatmap colors:
```{r}
# nn <- 6
# rc1 = colorRampPalette(colors = c("#543005", "#f5f5f5"), space="Lab")(nn)
# #rc1 = colorRampPalette(colors = c("magenta", "white"), space="Lab")(nn)
# ## Make vector of colors for values above threshold
# rc2 = colorRampPalette(colors = c("#f5f5f5", "#003c30"), space="Lab")(nn)
# rampcols = c(rc1, rc2[-1])

#rampcols <- RColorBrewer::brewer.pal(11,name = "BrBG")
rampcols <- viridis::viridis(20)

rampcols <- c("#7f3b08","#b35806","#e08214","#fdb863","#fee0b6","#f7f7f7","#d8daeb","#b2abd2","#8073ac","#542788","#2d004b")

# Min = min(Jall_new3)
# Max = max(Jall_new3)
Min = -1
Max = 1
Thresh = 0
# rb1 = seq(Min, Thresh, length.out=nn)
# rb2 = seq(Thresh, Max, length.out=nn)[-1]
# rampbreaks = c(rb1, rb2)

rampbreaks = seq(-1,1,0.1)
```

Try to produce ploidy-source composite factor with dummy variables and color mapping:
```{r}
test_meta$source_ploidy <- paste0(test_meta$source,'_',test_meta$ploidy)
test_meta$ID <- seq(1:length(test_meta[,1]))
test_meta2 <- cbind(test_meta, mm1 <- model.matrix(ID ~ source, data = test_meta)[,-1])

# define color scales:
colorz <- data.frame(matrix(c("#9ecae1","#3182bd","#a1d99b","#31a354","#fdae6b","#e6550d","#bcbddc","#756bb1","#fc9272","#de2d26","#fa9fb5","#c51b8a","#c994c7","#dd1c77","#8c96c6","#88419d","#fdcc8a","#fc8d59"), ncol = 9))
names(colorz) <- bfa.envs[-1]

# iterate through mm1 names and flag each 


```



Produce new heatmap:
```{r, fig.height=16, fig.width=6, message=FALSE, warning=FALSE}
# just turn all NAs to 0 for now
# test_input[is.na(test_input)] <- 0

heat2 <- heatmap.3(test_input[,-1],
                   hclustfun=myclust,
                   distfun=mydist,
                   na.rm = FALSE,
                   scale="col",
                   dendrogram="row",
                   margins=c(5,8),
                   Rowv=TRUE,
                   Colv=TRUE,
                   RowSideColors=rowcolz,
                   #breaks = rampbreaks,
                   #colsep = TRUE,
                   #rowsep=TRUE,
                   #sepcolor = 'gray30',
                   #sepwidth = c(0.025,0.025),
                   symbreaks=TRUE,
                   #key=TRUE,
                   symkey=TRUE,
                   density.info="none",
                   trace="none",
                   #main='dh8',
                   labCol=TRUE,
                   labRow = TRUE,
                   cexRow=0.33,
                   col=rampcols,
                   RowSideColorsSize=length(rowcolz[,1])#,
                   #KeyValueName="fitness effect (s)"
                   )
```

# trying this shit with `ggplot2` since this heatmap functionality blows.
```{r}
install.packages("plotly")
install.packages("heatmaply")
```


